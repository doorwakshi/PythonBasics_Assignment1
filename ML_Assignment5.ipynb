{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Logistic Regression is used for classification problems, where the output is categorical (e.g., 0 or 1), while Linear Regression is used for predicting continuous values. Logistic Regression uses the sigmoid function to map predictions to probabilities, whereas Linear Regression predicts raw continuous values.\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "The equation is:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùë¶\n",
        "=\n",
        "1\n",
        "‚à£\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "ùëí\n",
        "‚àí\n",
        "(\n",
        "ùõΩ\n",
        "0\n",
        "+\n",
        "ùõΩ\n",
        "1\n",
        "ùë•\n",
        "1\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùõΩ\n",
        "ùëõ\n",
        "ùë•\n",
        "ùëõ\n",
        ")\n",
        "P(y=1‚à£x)=\n",
        "1+e\n",
        "‚àí(Œ≤\n",
        "0\n",
        "‚Äã\n",
        " +Œ≤\n",
        "1\n",
        "‚Äã\n",
        " x\n",
        "1\n",
        "‚Äã\n",
        " +‚ãØ+Œ≤\n",
        "n\n",
        "‚Äã\n",
        " x\n",
        "n\n",
        "‚Äã\n",
        " )\n",
        "\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "The Sigmoid function maps any real-valued number to a range between 0 and 1, making it suitable for estimating probabilities in classification problems.\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "The cost function is the log loss (binary cross-entropy):\n",
        "\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "=\n",
        "‚àí\n",
        "1\n",
        "ùëö\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëö\n",
        "[\n",
        "ùë¶\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "‚Ñé\n",
        "(\n",
        "ùë•\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "‚Ñé\n",
        "(\n",
        "ùë•\n",
        "(\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(Œ∏)=‚àí\n",
        "m\n",
        "1\n",
        "‚Äã\n",
        "  \n",
        "i=1\n",
        "‚àë\n",
        "m\n",
        "‚Äã\n",
        " [y\n",
        "(i)\n",
        " log(h(x\n",
        "(i)\n",
        " ))+(1‚àíy\n",
        "(i)\n",
        " )log(1‚àíh(x\n",
        "(i)\n",
        " ))]\n",
        "5. What is regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Regularization adds a penalty to the loss function to avoid overfitting. It helps in reducing the model's complexity and improving generalization.\n",
        "\n",
        "6. What is the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Lasso (L1): Adds absolute value of coefficients; can shrink some to zero (feature selection).\n",
        "\n",
        "Ridge (L2): Adds squared value of coefficients; shrinks but doesn't eliminate.\n",
        "\n",
        "Elastic Net: Combines both L1 and L2.\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Use Elastic Net when there are many correlated features. It balances the benefits of both Lasso and Ridge.\n",
        "\n",
        "8. What is the impact of the regularization parameter (Œª) in Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Œª controls the strength of regularization. High Œª means more penalty (simpler model), while low Œª may cause overfitting.\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Linear relationship between log-odds and independent variables\n",
        "\n",
        "No multicollinearity\n",
        "\n",
        "Large sample size\n",
        "\n",
        "Independence of observations\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "SVM\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "KNN\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-Score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "It can lead to biased predictions toward the majority class. Solutions include resampling, using class weights, or alternate metrics like F1-score.\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Tuning values like C (inverse of regularization strength), solver type, and max iterations to improve model performance.\n",
        "\n",
        "14. What are the different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Solution:\n",
        "\n",
        "liblinear: For small datasets and L1 penalty\n",
        "\n",
        "lbfgs: For L2 regularization and multiclass\n",
        "\n",
        "saga: Supports both L1 and L2; good for large datasets\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Using One-vs-Rest (OvR) or Softmax (multinomial logistic regression).\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Advantages: Simple, interpretable, efficient\n",
        "Disadvantages: Assumes linearity, sensitive to outliers, not suitable for complex relationships\n",
        "\n",
        "17. What are some use cases of Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Spam detection\n",
        "\n",
        "Disease diagnosis\n",
        "\n",
        "Credit scoring\n",
        "\n",
        "Marketing campaign response prediction\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Logistic Regression handles binary classification; Softmax handles multiclass by outputting probability for each class.\n",
        "\n",
        "19. How to choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Solution:\n",
        "\n",
        "OvR: Simpler, faster for fewer classes\n",
        "\n",
        "Softmax: Better for mutually exclusive classes and larger datasets\n",
        "\n",
        "20. How to interpret coefficients in Logistic Regression?\n",
        "\n",
        "Solution:\n",
        "\n",
        "Each coefficient represents the log-odds change in the outcome for a unit change in the predictor.\n",
        "Exponentiating gives the odds ratio."
      ],
      "metadata": {
        "id": "Txbit8c2-Wi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L2 Regularization Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n",
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"ElasticNet Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Multiclass OVR Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "\n",
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(model, data.data, data.target, cv=cv)\n",
        "print(\"Average Cross-Validation Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMb3WnKT_Z0y",
        "outputId": "7681ad1d-dff6-4310-e03e-ce1bb0b498e3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "L1 Regularization Accuracy: 1.0\n",
            "L2 Regularization Accuracy: 1.0\n",
            "Coefficients: [[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n",
            "ElasticNet Accuracy: 1.0\n",
            "Multiclass OVR Accuracy: 1.0\n",
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9583333333333334\n",
            "Average Cross-Validation Accuracy: 0.9733333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "# Practical 8: Logistic Regression from CSV Dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file\n",
        "# Make sure to change the file path to your actual CSV file name\n",
        "# Replace 'your_dataset.csv' with the actual file name or path\n",
        "# If the file is in a different directory, specify the full path.\n",
        "# For example: df = pd.read_csv(\"/path/to/your/data.csv\")\n",
        "df = pd.read_csv(\"iris.csv\")  # Example: Replace with your file name or path\n",
        "\n",
        "# Display first few rows to understand the structure\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in dataset:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing values (if any)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Convert categorical values if needed\n",
        "# df = pd.get_dummies(df, drop_first=True)  # Uncomment if categorical data is present\n",
        "\n",
        "# Assume the last column is the target variable\n",
        "X = df.iloc[:, :-1]  # Features (all columns except last)\n",
        "y = df.iloc[:, -1]   # Target (last column)\n",
        "\n",
        "# Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model\n"
      ],
      "metadata": {
        "id": "LmG8VFQWPnVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression # Importing LogisticRegression\n",
        "\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 2, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=200), param_distributions=param_dist, n_iter=10, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "\n",
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "print(\"OvO Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)\n",
        "\n",
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Balanced Class Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "df = sns.load_dataset('titanic')\n",
        "df = df[['age', 'fare', 'sex', 'survived']].dropna()\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "X = df[['age', 'fare', 'sex']]\n",
        "y = df['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Titanic Dataset Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train_scaled, X_test_scaled, _, _ = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "print(\"Accuracy with scaling:\", model_scaled.score(X_test_scaled, y_test))\n",
        "print(\"Accuracy without scaling:\", model.score(X_test, y_test))\n",
        "\n",
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
        "y_score = model.decision_function(X_test)\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_bin, y_score))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "R4PlYVVO_bEt",
        "outputId": "6725e72f-76fd-4958-927e-d2940f69978c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': np.float64(7.742636826811277)}\n",
            "Best Accuracy: 0.9666666666666668\n",
            "OvO Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Balanced Class Accuracy: 1.0\n",
            "Titanic Dataset Accuracy: 0.7482517482517482\n",
            "Accuracy with scaling: 0.7482517482517482\n",
            "Accuracy without scaling: 0.7482517482517482\n",
            "ROC-AUC Score: 0.7475369458128078\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Custom C Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feuMoFW4NYGR",
        "outputId": "c68060e1-9054-4fc8-eed6-972d08b0f69d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom C Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature names from the original DataFrame (df) if available\n",
        "# If df is not available, you can manually define the feature names if known\n",
        "# Replace ['feature1', 'feature2', 'feature3'] with your actual feature names\n",
        "feature_names = df.columns[:-1]  # Assuming the last column is the target\n",
        "# Or:\n",
        "# feature_names = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
        "\n",
        "for feature, coef in zip(feature_names, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhdZVb98OAhm",
        "outputId": "7f1365ae-4ed1-45d5-f3d6-d757b5cf4856"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: -0.39345606847680076\n",
            "fare: 0.96251768326157\n",
            "sex: -2.375124360817527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score.\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "print(\"Cohen's Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNj4tzTMNYua",
        "outputId": "b100f09b-b51a-4099-f528-b6fdf6bc7fe5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Assuming you want to plot the curve for class '0' (adjust if needed)\n",
        "pos_label = 0  # Choose the positive class label for binary analysis\n",
        "\n",
        "# Binarize the target variable for the chosen class\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))[:, pos_label]\n",
        "\n",
        "# Get prediction scores for the chosen class\n",
        "y_scores = model.decision_function(X_test)[:, pos_label]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test_bin, y_scores)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "NRxUPepwNZDz",
        "outputId": "25bd3d66-7bb5-4308-b6f3-6d65da6f377f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x78be11d506d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "for solver in ['liblinear', 'saga', 'lbfgs']:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Solver {solver}: Accuracy = {model.score(X_test, y_test)}\")\n",
        "\n",
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCnMTHXNNZej",
        "outputId": "109c8761-f104-409d-cb33-0b60689ef659"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver liblinear: Accuracy = 1.0\n",
            "Solver saga: Accuracy = 1.0\n",
            "Solver lbfgs: Accuracy = 1.0\n",
            "Matthews Correlation Coefficient: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "acc_raw = model_raw.score(X_test, y_test)\n",
        "\n",
        "# Scale X_train and X_test instead of the whole dataset\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) #Only transform X_test\n",
        "\n",
        "model_std = LogisticRegression(max_iter=200)\n",
        "model_std.fit(X_train_scaled, y_train)\n",
        "acc_std = model_std.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Raw Accuracy:\", acc_raw)\n",
        "print(\"Standardized Accuracy:\", acc_std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgYenQl6Pe4c",
        "outputId": "3dbafd57-5dff-4c25-c785-8b92a7d21ce6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Accuracy: 1.0\n",
            "Standardized Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10]\n",
        "for c in C_values:\n",
        "    model = LogisticRegression(C=c, max_iter=200)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"C={c}, Mean CV Accuracy={scores.mean()}\")\n",
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(model, 'logreg_model.pkl')\n",
        "loaded_model = joblib.load('logreg_model.pkl')\n",
        "print(\"Loaded Model Accuracy:\", loaded_model.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "KbMNcP3jNZ3U",
        "outputId": "b5250329-9721-45c5-ad4b-8c235711d276"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.01, Mean CV Accuracy=0.86\n",
            "C=0.1, Mean CV Accuracy=0.9466666666666667\n",
            "C=1, Mean CV Accuracy=0.9733333333333334\n",
            "C=10, Mean CV Accuracy=0.9733333333333334\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3d9ddfdb5373>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logreg_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logreg_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded Model Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \"\"\"\n\u001b[1;32m    373\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexing_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mthis\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    }
  ]
}