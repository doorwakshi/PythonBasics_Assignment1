{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Questions (1–15)\n",
        "1. What is a Decision Tree, and how does it work?\n",
        "\n",
        "Answer:\n",
        "\n",
        "A Decision Tree is a supervised learning algorithm used for classification and regression. It splits data into branches using features that best separate the target classes based on impurity measures like Gini or Entropy.\n",
        "\n",
        "2. What are impurity measures in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Impurity measures determine how well a feature splits the data:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Entropy\n",
        "\n",
        "Information Gain (based on Entropy)\n",
        "\n",
        "3. What is the mathematical formula for Gini impurity?\n",
        "\n",
        "Answer:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "Where\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of class\n",
        "𝑖\n",
        "i.\n",
        "\n",
        "4. What is the mathematical formula for Entropy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "\n",
        "5. What is Information Gain, and how is it used in Decision Trees?\n",
        "Information Gain is the reduction in entropy after a dataset is split on an attribute.\n",
        "\n",
        "Answer:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "−\n",
        "∑\n",
        "(\n",
        "∣\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "∣\n",
        "∣\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "∣\n",
        "⋅\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        ")\n",
        ")\n",
        "IG=Entropy(parent)−∑(\n",
        "∣parent∣\n",
        "∣child∣\n",
        "​\n",
        " ⋅Entropy(child))\n",
        "\n",
        "6. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gini is faster to compute.\n",
        "\n",
        "Entropy involves logarithmic calculations.\n",
        "\n",
        "Both measure impurity, but Gini tends to isolate the most frequent class faster.\n",
        "\n",
        "7. What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "It involves recursively choosing features based on highest Information Gain or lowest Gini to partition the data into subsets, creating a tree structure.\n",
        "\n",
        "8. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Stopping tree growth early using criteria like max_depth or min_samples_split.\n",
        "\n",
        "9. What is Post-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Letting the tree fully grow and then trimming unnecessary branches using a validation set.\n",
        "\n",
        "10. What is the difference between Pre-Pruning and Post-Pruning?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-Pruning prevents overfitting during training.\n",
        "\n",
        "Post-Pruning removes overfitted parts after full tree creation.\n",
        "\n",
        "11. What is a Decision Tree Regressor?\n",
        "\n",
        "Answer:\n",
        "\n",
        "A Decision Tree used for predicting continuous values (regression tasks) instead of classes.\n",
        "\n",
        "12. What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to understand\n",
        "\n",
        "No data normalization needed\n",
        "\n",
        "Works for both classification and regression\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Less accurate than ensemble models\n",
        "\n",
        "Can be unstable with small changes in data\n",
        "\n",
        "13. How does a Decision Tree handle missing values?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Scikit-learn doesn't natively support missing values in trees. Options:\n",
        "\n",
        "Imputation before training\n",
        "\n",
        "Use models like XGBoost that handle missing data internally\n",
        "\n",
        "14. How does a Decision Tree handle categorical features?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Convert categories to numerical values (e.g., One-Hot Encoding or Label Encoding).\n",
        "\n",
        "Scikit-learn requires numeric inputs.\n",
        "\n",
        "15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Medical Diagnosis\n",
        "\n",
        "Customer Churn Prediction\n",
        "\n",
        "Credit Scoring\n",
        "\n",
        "Loan Approval\n",
        "\n",
        "Fraud Detection"
      ],
      "metadata": {
        "id": "KQ4Q6SFSDHay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Practical Questions (Q16 to Q27)\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree, DecisionTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Q16 - Basic Decision Tree Classifier\n",
        "\n",
        "clf_16 = DecisionTreeClassifier()\n",
        "clf_16.fit(X_train, y_train)\n",
        "y_pred_16 = clf_16.predict(X_test)\n",
        "print(\"Q16 - Accuracy:\", accuracy_score(y_test, y_pred_16))\n",
        "\n",
        "# Q17 - Using Gini Impurity\n",
        "\n",
        "clf_17 = DecisionTreeClassifier(criterion=\"gini\")\n",
        "clf_17.fit(X_train, y_train)\n",
        "print(\"Q17 - Feature Importances:\", clf_17.feature_importances_)\n",
        "\n",
        "# Q18 - Using Entropy\n",
        "\n",
        "clf_18 = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "clf_18.fit(X_train, y_train)\n",
        "y_pred_18 = clf_18.predict(X_test)\n",
        "print(\"Q18 - Accuracy:\", accuracy_score(y_test, y_pred_18))\n",
        "\n",
        "#Q19: Train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)\n",
        "#We'll use the California Housing dataset from sklearn.datasets.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Q19 - Mean Squared Error on housing dataset:\", mse)\n",
        "\n",
        "# Q20 - Visualize tree using graphviz (text form)\n",
        "\n",
        "dot_data = export_graphviz(clf_16, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names)\n",
        "print(\"Q20 - DOT data created for Graphviz visualization\")\n",
        "\n",
        "# Q21 - Max Depth 3 vs Full Tree\n",
        "\n",
        "clf_21_full = DecisionTreeClassifier()\n",
        "clf_21_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_21_full.predict(X_test))\n",
        "\n",
        "clf_21_limited = DecisionTreeClassifier(max_depth=3)\n",
        "clf_21_limited.fit(X_train, y_train)\n",
        "acc_limited = accuracy_score(y_test, clf_21_limited.predict(X_test))\n",
        "print(\"Q21 - Full Accuracy:\", acc_full, \"| MaxDepth=3 Accuracy:\", acc_limited)\n",
        "\n",
        "# Q22 - min_samples_split=5 vs default\n",
        "\n",
        "clf_22_default = DecisionTreeClassifier()\n",
        "clf_22_default.fit(X_train, y_train)\n",
        "acc_def = accuracy_score(y_test, clf_22_default.predict(X_test))\n",
        "\n",
        "clf_22_custom = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_22_custom.fit(X_train, y_train)\n",
        "acc_custom = accuracy_score(y_test, clf_22_custom.predict(X_test))\n",
        "print(\"Q22 - Default Accuracy:\", acc_def, \"| min_samples_split=5 Accuracy:\", acc_custom)\n",
        "\n",
        "# Q23 - Feature scaling before training\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_23 = DecisionTreeClassifier()\n",
        "clf_23.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, clf_23.predict(X_test_scaled))\n",
        "print(\"Q23 - Accuracy without Scaling:\", acc_def, \"| With Scaling:\", acc_scaled)\n",
        "\n",
        "# Q24 - One-vs-Rest (OvR)\n",
        "\n",
        "clf_24 = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "clf_24.fit(X_train, y_train)\n",
        "print(\"Q24 - OvR Accuracy:\", accuracy_score(y_test, clf_24.predict(X_test)))\n",
        "\n",
        "# Q25 - Feature Importance Display\n",
        "\n",
        "clf_25 = DecisionTreeClassifier()\n",
        "clf_25.fit(X_train, y_train)\n",
        "print(\"Q25 - Feature Importances:\", clf_25.feature_importances_)\n",
        "\n",
        "# Q26 - Decision Tree Regressor with max_depth=5\n",
        "\n",
        "X_reg = np.random.rand(100, 1) * 10\n",
        "y_reg = np.sin(X_reg).ravel()\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "reg_full = DecisionTreeRegressor()\n",
        "reg_full.fit(X_reg_train, y_reg_train)\n",
        "mse_full = mean_squared_error(y_reg_test, reg_full.predict(X_reg_test))\n",
        "\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5)\n",
        "reg_limited.fit(X_reg_train, y_reg_train)\n",
        "mse_limited = mean_squared_error(y_reg_test, reg_limited.predict(X_reg_test))\n",
        "print(\"Q26 - MSE Unrestricted:\", mse_full, \"| MSE MaxDepth=5:\", mse_limited)\n",
        "\n",
        "# Q27 - Cost Complexity Pruning\n",
        "\n",
        "path = clf_16.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "acc_list = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "    acc_list.append((ccp_alpha, acc))\n",
        "print(\"Q27 - Accuracy with different CCP alphas:\")\n",
        "for alpha, acc in acc_list:\n",
        "    print(f\"Alpha: {alpha:.4f}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "m8nRKRmVH6cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}